{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspectral Image Classification with Hybrid Hypergraph Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.segmentation import slic\n",
    "\n",
    "from src.models.hybrid_hypergraph_attention import build_model, create_hypergraph_adjacency_matrix\n",
    "from src.utils.data_loader import load_data\n",
    "from src.utils.preprocessing import normalize_data, apply_pca, segment_image, preprocess_data\n",
    "from src.utils.evaluation import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "data, labels = load_data('data/IP/IP.mat')\n",
    "print(\"Dataset loaded. Data shape:\", data.shape, \"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization\n",
    "print(\"Visualizing data...\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(labels)\n",
    "plt.title('Class Distribution in the Dataset')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "print(\"Normalizing data...\")\n",
    "data_normalized = normalize_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for dimensionality reduction\n",
    "print(\"Applying PCA...\")\n",
    "data_pca = apply_pca(data_normalized, n_components=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLIC for superpixel segmentation\n",
    "print(\"Segmenting data using SLIC...\")\n",
    "segments = segment_image(data_pca, n_segments=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample of the segmented data\n",
    "print(\"Visualizing segmented data...\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(segments, cmap='gray')\n",
    "plt.title('Sample Segmented Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "print(\"Splitting dataset into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(segments, labels, test_size=0.2, random_state=42)\n",
    "print(\"Dataset split. Training data shape:\", X_train.shape, \"Testing data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hypergraph adjacency matrix for training data\n",
    "print(\"Creating hypergraph adjacency matrix for training data...\")\n",
    "adjacency_matrix_train = create_hypergraph_adjacency_matrix(X_train, n_superpixels=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hypergraph adjacency matrix for testing data\n",
    "print(\"Creating hypergraph adjacency matrix for testing data...\")\n",
    "adjacency_matrix_test = create_hypergraph_adjacency_matrix(X_test, n_superpixels=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "print(\"Building the model...\")\n",
    "model = build_model(input_dim=30, hidden_dim=64, output_dim=len(np.unique(labels)))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Model built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Training the model...\")\n",
    "model.train()\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(torch.tensor(X_train, dtype=torch.float32), adjacency_matrix_train)\n",
    "    loss = criterion(outputs, torch.tensor(y_train, dtype=torch.long))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "print(\"Plotting training loss...\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "print(\"Evaluating the model...\")\n",
    "oa, aa, kappa = evaluate_model(model, X_test, y_test, adjacency_matrix_test)\n",
    "print(f'Evaluation results - OA: {oa}, AA: {aa}, Kappa: {kappa}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"Detailed classification report...\")\n",
    "from sklearn.metrics import classification_report\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(torch.tensor(X_test, dtype=torch.float32), adjacency_matrix_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    predicted = predicted.numpy()\n",
    "    y_test = np.array(y_test)\n",
    "    print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning example (Learning Rate)\n",
    "print(\"Hyperparameter tuning example (Learning Rate)...\")\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "tuning_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = build_model(input_dim=30, hidden_dim=64, output_dim=len(np.unique(labels)))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(torch.tensor(X_train, dtype=torch.float32), adjacency_matrix_train)\n",
    "        loss = criterion(outputs, torch.tensor(y_train, dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    oa, aa, kappa = evaluate_model(model, X_test, y_test, adjacency_matrix_test)\n",
    "    tuning_results.append((lr, oa, aa, kappa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting hyperparameter tuning results\n",
    "print(\"Plotting hyperparameter tuning results...\")\n",
    "learning_rates, oa_scores, aa_scores, kappa_scores = zip(*tuning_results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(learning_rates, oa_scores, label='Overall Accuracy')\n",
    "plt.plot(learning_rates, aa_scores, label='Average Accuracy')\n",
    "plt.plot(learning_rates, kappa_scores, label='Kappa')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Hyperparameter Tuning (Learning Rate)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation study (Removing PCA)\n",
    "print(\"Ablation study (Removing PCA)...\")\n",
    "segments_no_pca = segment_image(normalize_data(data), n_segments=100)\n",
    "X_train_no_pca, X_test_no_pca, y_train_no_pca, y_test_no_pca = train_test_split(segments_no_pca, labels, test_size=0.2, random_state=42)\n",
    "adjacency_matrix_train_no_pca = create_hypergraph_adjacency_matrix(X_train_no_pca, n_superpixels=100)\n",
    "adjacency_matrix_test_no_pca = create_hypergraph_adjacency_matrix(X_test_no_pca, n_superpixels=100)\n",
    "\n",
    "model_no_pca = build_model(input_dim=data.shape[-1], hidden_dim=64, output_dim=len(np.unique(labels)))\n",
    "optimizer_no_pca = optim.Adam(model_no_pca.parameters(), lr=0.001)\n",
    "model_no_pca.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer_no_pca.zero_grad()\n",
    "    outputs = model_no_pca(torch.tensor(X_train_no_pca, dtype=torch.float32), adjacency_matrix_train_no_pca)\n",
    "    loss = criterion(outputs, torch.tensor(y_train_no_pca, dtype=torch.long))\n",
    "    loss.backward()\n",
    "    optimizer_no_pca.step()\n",
    "\n",
    "oa_no_pca, aa_no_pca, kappa_no_pca = evaluate_model(model_no_pca, X_test_no_pca, y_test_no_pca, adjacency_matrix_test_no_pca)\n",
    "print(f'Ablation study results (Without PCA) - OA: {oa_no_pca}, AA: {aa_no_pca}, Kappa: {kappa_no_pca}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
